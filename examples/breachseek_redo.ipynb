{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f68bdf5",
   "metadata": {},
   "source": [
    "### BreachSeek Redo\n",
    "\n",
    "<img src=\"https://arxiv.org/html/2409.03789v1/extracted/5823759/Graph.png\" width=500px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ec2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/model.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def _get_model(model_name: str = \"openai\"):\n",
    "    # if model_name == \"anthropic\":\n",
    "    #     model = ChatAnthropic(temperature=0, model_name=\"claude-3-5-sonnet-20240620\")\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fbb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/tools.py\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools import ShellTool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [DuckDuckGoSearchRun(), ShellTool()]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a21c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/state.py\n",
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import AnyMessage, BaseMessage\n",
    "from typing_extensions import TypedDict, Annotated, Sequence, List,  Literal, Optional, Dict\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "class RecoderOptions(TypedDict):\n",
    "    report: str\n",
    "    generate_final_report: bool\n",
    "    file_names: List[str]\n",
    "\n",
    "\n",
    "class PentestState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], operator.add]\n",
    "    current_step: Literal['pentester', 'evaluator', 'recorder', '__end__']\n",
    "    pentest_results: dict\n",
    "    pentest_tasks: list\n",
    "    task: str \n",
    "    evaluation: str \n",
    "    model_status: str\n",
    "    findings: Dict[str, List[str]]\n",
    "    command: str\n",
    "    tool_name: str\n",
    "    tool_results: str \n",
    "    recorder_options: Optional[RecoderOptions]\n",
    "\n",
    "\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "    pentester_model: Literal[\"ollama\",\" anthropic\", \"openai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/nodes.py\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Be a helpful agent, you can search the web and you can run shell commands.\n",
    "If you receive the output from a tool\n",
    "show the output of the commands in markdown for debugging purposes prefix your\n",
    "response after the tool call with DEBUG: keyword.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get('configurable', {}).get(\"model_name\", \"openai\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# supervisor.py\n",
    "supervisor_system_prompt = SystemMessage(\"\"\"\n",
    "You are the supervisor of AI agents responsible for overseeing their performance, \n",
    "you send the pentesting tasks to pentester agent, and you receive the response \n",
    "from an evaluator agent, if the evaluator says everything is done, then end the program,\n",
    "\n",
    "Try to end as soon as possible. \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class Report(TypedDict):\n",
    "    report: Annotated[str, \"This is the summary written in Latex\"]\n",
    "    file_name: Annotated[str, \"This is the filename of the Latex report\"]\n",
    "\n",
    "\n",
    "# Don't think we need this one\n",
    "def _swap_messages(messages):\n",
    "    new_messages = []\n",
    "    for m in messages:\n",
    "        if m['role'] == 'assistant':\n",
    "            new_messages.append({\"role\": \"user\", \"content\": m['content']})\n",
    "        else:\n",
    "            new_messages.append({\"role\": \"assistant\", \"content\": m['content']})\n",
    "    return new_messages\n",
    "\n",
    "\n",
    "def supervisor(state: PentestState) -> PentestState:\n",
    "    # If all evaluator concludes all tasks are done -> Termination\n",
    "    if state['evaluation'] and 'end' in state['evaluation']:\n",
    "        return {\n",
    "                'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "                'current_step': '__end__',\n",
    "                }\n",
    "\n",
    "\n",
    "    messages = [system_prompt] + state['messages']\n",
    "\n",
    "    # Pentester ran, and evaluation has been made (Yet done)\n",
    "    if state['evaluation']:\n",
    "        messages = [system_prompt] + [HumanMessage(content=f\"based on {state['evaluation']}, what should we do now to finish the tasks: {state['pentest_tasks']}\")]\n",
    "    # messages = [system_prompt] + state['messages']\n",
    "    model = _get_model().with_structured_output(SupervisorTask)\n",
    "    response = model.invoke(messages) # error here\n",
    "    # If supervisor concludes all tasks are done -> Termination\n",
    "    if response['done']:\n",
    "        return {\n",
    "                'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "                'current_step': '__end__',\n",
    "                }\n",
    "\n",
    "    # In case the pentester should be run\n",
    "    state['current_step'] = response['next_agent']\n",
    "    if response['next_agent'] == 'pentester':\n",
    "        return {\n",
    "                'messages': [{'role': 'assistant', \n",
    "                             'content': response['supervisor_thought']}],\n",
    "                'current_step': 'pentester',\n",
    "                'pentest_tasks': response['tasks'],\n",
    "                }\n",
    "\n",
    "    # Base condition (?)\n",
    "    return {\n",
    "                'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "                'current_step': '__end__',\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261e1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorder.py\n",
    "class Report(TypedDict):\n",
    "    report: Annotated[str, \"This is the summary written in Latex\"]\n",
    "    file_name: Annotated[str, \"This is the filename of the Latex report\"]\n",
    "\n",
    "\n",
    "\n",
    "def recorder_summary(state: PentestState, model):\n",
    "    prompt = '''You are tasked with recording and summarizing information of a chat between a human and an LLM in \\\n",
    "    which the LLM is utilizing the command line to execute tasks. You have as an input the history of the last command that ran \\\n",
    "    which inclueds the output logs and the message prompt for previous commands:\n",
    "     \n",
    "    <history>\n",
    "    {history}\n",
    "    </history>\n",
    "\n",
    "    Generate a summary for this using latex.\n",
    "    '''\n",
    "    formats = {\n",
    "            'history': {'user_prompts': state['messages']} | state['pentest_results'],\n",
    "        }\n",
    "    prompt = prompt.format(**formats)\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    state['recorder_options']['file_names'].append(response.file_name)\n",
    "    \n",
    "    with open(response.file_name, 'w') as f:\n",
    "        f.write(response.report)\n",
    "\n",
    "    return state\n",
    "\n",
    "def recorder_final(state: PentestState, model):\n",
    "    prompt = '''You are tasked with summarizing and reporting information of a chat between a human and an LLM in \\\n",
    "    whihc the LLM is utilizing the command line to execute tasks. You have as input the history of summaries of all previous \\\n",
    "    outputs and interactions between the human and the LLM:\n",
    "\n",
    "    <history>\n",
    "    {history}\n",
    "    </history>\n",
    "\n",
    "    You are tasked with generating a final report in a Latex format. Also return the file path as a relative path.\n",
    "    '''\n",
    "\n",
    "    summaries = []\n",
    "    file_names = state['recorder_options']['file_names']\n",
    "    for file_name in file_names:\n",
    "        with open(file_name) as f:\n",
    "            summary = f.read()\n",
    "            summaries.append(summary)\n",
    "\n",
    "    formats = {\n",
    "            'history': summaries\n",
    "        }\n",
    "    prompt = prompt.format(**formats)\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    state['recorder_options']['report'] = response.report\n",
    "    with open(response.file_name, 'w') as f:\n",
    "        f.write(response.report)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def recorder(state: PentestState):\n",
    "    model = _get_model().with_structured_output(Report)\n",
    "    generate_final_report = state['recorder_options']['generate_final_report']\n",
    "\n",
    "    if generate_final_report:\n",
    "        return recorder_final(state, model)\n",
    "    else:\n",
    "        state = recorder_summary(state, model)\n",
    "        return recorder_final(state, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f08e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentester.py\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from typing_extensions import Annotated, TypedDict, List, Literal, Optional \n",
    "\n",
    "\n",
    "class PentesterResults(TypedDict):\n",
    "    \"\"\"The results that the pentester needs to work\"\"\"\n",
    "    message: Annotated[str, ..., \"Pentester thought process\"]\n",
    "    phase: Annotated[str, ..., \"Current phase of pentesting, such as scanning or exploitation\"]\n",
    "    tasks: Annotated[List[str], ..., \"Tasks to perform to complete the current phase\"]\n",
    "    results: Annotated[List[str], ..., \"Summary of results\"]\n",
    "    tool_use: Annotated[bool, ..., \"Indicates if tools are still needed to complete a task\"]\n",
    "    software: Annotated[List[str], ..., \"program names that will be used to perform the tasks\"]\n",
    "    command: Annotated[str, ..., \"possible command that will be run\"]\n",
    "\n",
    "\n",
    "def _swap_messages(messages):\n",
    "    new_messages = []\n",
    "    for m in messages:\n",
    "        if m['role'] == 'assistant':\n",
    "            new_messages.append({\"role\": \"user\", \"content\": m['content']})\n",
    "        else:\n",
    "            new_messages.append({\"role\": \"assistant\", \"content\": m['content']})\n",
    "    return new_messages\n",
    "\n",
    "\n",
    "def pentester(state: PentestState) -> PentestState:\n",
    "    # print(state)\n",
    "    tasks = f\"\"\"\n",
    "    Do the following tasks:\n",
    "\n",
    "    {state['pentest_tasks']}\n",
    "    \"\"\"\n",
    "\n",
    "    if state['tool_results']: # remove later, for debugging\n",
    "        return {'current_step': 'evaluator'}\n",
    "\n",
    "    # messages = [system_prompt, HumanMessage(content=tasks)] \n",
    "    messages = [system_prompt] + [{'role': 'user', 'content': tasks}] + _swap_messages(state['messages'])\n",
    "    model = _get_model().with_structured_output(PentesterResults)\n",
    "    response = model.invoke(messages)\n",
    "    print(f'pentester:\\n{response = }')\n",
    "\n",
    "    # In case that the tool needs to be used\n",
    "    if response['tool_use']:\n",
    "        return {\n",
    "                'messages': [{'role': 'assistant',\n",
    "                              'content': response['message']}],\n",
    "                              # 'tool_calls': response['software']}], \n",
    "                'current_step': 'pentester',\n",
    "                'tool_name': response['software'][0],\n",
    "                'task': response['tasks'][0],\n",
    "                # \"command\": \"nmap -sV -sC -p- -oN metasploitable2_scan.txt 192.168.100.231\",\n",
    "                \"command\": response['command'],\n",
    "                }\n",
    "    return {'current_step': 'evaluator'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1689b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools.py\n",
    "class ToolUsage(TypedDict):\n",
    "    \"\"\"The task that the tool needs to do\"\"\"\n",
    "    message: str\n",
    "    # phase: Annotated[str, ... , \"Current phase of pentesting\"]\n",
    "    # task: Annotated[str, ... , \"Task to perform so that the tool agent succeeds.\"]\n",
    "    # hints: Annotated[Optional[List[str]], ... , \"May contain hints about the task\"]\n",
    "    # constraints: Annotated[Optional[List[str]], ... , \"Contains constraints that the tool user agent should be aware of\"]\n",
    "    # program_name: Annotated[str, ... , \"Program name that the tool uses to finish a task\"]\n",
    "    # args: Annotated[Optional[Dict], ... , \"Args that might help the tool user agent\"]\n",
    "    # results: Annotated[Dict, ... , \"Title of findings as keys, and a description as their value\"]\n",
    "    results: str\n",
    "\n",
    "\n",
    "def tools_node(state: PentestState) -> PentestState:\n",
    "    system_prompt = SystemMessage(f\"\"\"\n",
    "    You are an AI agent who is an expert in using a tool named {state['tool_name']}, \n",
    "    to do {state['pentest_tasks']}. You are working with a team of AI agents, \n",
    "    you will receive a task, and possibly hints, constrains, and args. \n",
    "    You will have to use your expertise in {state['tool_name']} to finish\n",
    "    the task. When you are done pass the results to the calling agent. \n",
    "\n",
    "    you have access to bash shell using shell_tool\n",
    "    \"\"\")\n",
    "    print(f'{system_prompt = }')\n",
    "    human_msg = HumanMessage(content=f\"Use {state['tool_name']} to do {state['task']}\")\n",
    "    messages = [system_prompt] + [human_msg]\n",
    "    model = _get_model().bind_tools(tools)\n",
    "    ai_msg = model.invoke(messages)\n",
    "    messages.append(ai_msg)\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        selected_tool = {tool.name: tool for tool in tools}[tool_call[\"name\"].lower()]\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        # tool_msg = selected_tool.invoke(tool_call)\n",
    "        messages.append(tool_msg)\n",
    "    # tool_msg = shell_tool.invoke(ai_msg.tool_calls)\n",
    "    # messages.append(tool_msg)\n",
    "    messages.append(HumanMessage(content='summarize before parsing'))\n",
    "    model = _get_model().with_structured_output(ToolUsage)\n",
    "    response = model.invoke(messages)\n",
    "    print(f'tools:\\n{response = }')\n",
    "    return {\n",
    "            'messages': [{'role': 'assistant',\n",
    "                          'content': '\\n\\n'.join(response.values())}],\n",
    "            'tool_results': response['results'],\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
