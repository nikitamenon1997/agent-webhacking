{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f68bdf5",
   "metadata": {},
   "source": [
    "### BreachSeek Redo\n",
    "\n",
    "<img src=\"https://arxiv.org/html/2409.03789v1/extracted/5823759/Graph.png\" width=500px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb25d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2ec2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/model.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def _get_model(model_name: str = \"openai\"):\n",
    "    # if model_name == \"anthropic\":\n",
    "    #     model = ChatAnthropic(temperature=0, model_name=\"claude-3-5-sonnet-20240620\")\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41fbb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/tools.py\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools import ShellTool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [DuckDuckGoSearchRun(), ShellTool()]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a21c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/state.py\n",
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import AnyMessage, BaseMessage\n",
    "from typing_extensions import TypedDict, Annotated, Sequence, List,  Literal, Optional, Dict\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "class RecoderOptions(TypedDict):\n",
    "    report: str\n",
    "    generate_final_report: bool\n",
    "    file_names: List[str]\n",
    "\n",
    "\n",
    "class PentestState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], operator.add]\n",
    "    current_step: Literal['pentester', 'evaluator', 'recorder', '__end__']\n",
    "    pentest_results: dict\n",
    "    pentest_tasks: list\n",
    "    task: str \n",
    "    evaluation: str \n",
    "    model_status: str\n",
    "    findings: Dict[str, List[str]]\n",
    "    command: str\n",
    "    tool_name: str\n",
    "    tool_results: str \n",
    "    recorder_options: Optional[RecoderOptions]\n",
    "\n",
    "\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "    pentester_model: Literal[\"ollama\",\" anthropic\", \"openai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc97eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/nodes.py\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Be a helpful agent, you can search the web and you can run shell commands.\n",
    "If you receive the output from a tool\n",
    "show the output of the commands in markdown for debugging purposes prefix your\n",
    "response after the tool call with DEBUG: keyword.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get('configurable', {}).get(\"model_name\", \"openai\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c2a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# supervisor.py\n",
    "supervisor_system_prompt = SystemMessage(\"\"\"\n",
    "You are the supervisor of AI agents responsible for overseeing their performance, \n",
    "you send the pentesting tasks to pentester agent, and you receive the response \n",
    "from an evaluator agent, if the evaluator says everything is done, then end the program,\n",
    "\n",
    "Try to end as soon as possible. \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class SupervisorTask(TypedDict):\n",
    "    supervisor_thought: Annotated[str, ..., \"supervisor thoughts that includes decision making based on evaluator prompt if he responded\"]\n",
    "    tasks: list\n",
    "    next_agent: Literal['pentester', 'evaluator', 'recorder', '__end__']\n",
    "    done: bool\n",
    "\n",
    "\n",
    "\n",
    "# Don't think we need this one\n",
    "def _swap_messages(messages):\n",
    "    new_messages = []\n",
    "    for m in messages:\n",
    "        if m['role'] == 'assistant':\n",
    "            new_messages.append({\"role\": \"user\", \"content\": m['content']})\n",
    "        else:\n",
    "            new_messages.append({\"role\": \"assistant\", \"content\": m['content']})\n",
    "    return new_messages\n",
    "\n",
    "\n",
    "# def supervisor(state: PentestState) -> PentestState:\n",
    "#     # If all evaluator concludes all tasks are done -> Termination\n",
    "#     if state['evaluation'] and 'end' in state['evaluation']:\n",
    "#         return {\n",
    "#                 'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "#                 'current_step': '__end__',\n",
    "#                 }\n",
    "\n",
    "\n",
    "#     messages = [system_prompt] + state['messages']\n",
    "\n",
    "#     # Pentester ran, and evaluation has been made (Yet done)\n",
    "#     if state['evaluation']:\n",
    "#         messages = [system_prompt] + [HumanMessage(content=f\"based on {state['evaluation']}, what should we do now to finish the tasks: {state['pentest_tasks']}\")]\n",
    "#     # messages = [system_prompt] + state['messages']\n",
    "#     model = _get_model().with_structured_output(SupervisorTask)\n",
    "#     response = model.invoke(messages) # error here\n",
    "#     # If supervisor concludes all tasks are done -> Termination\n",
    "#     if response['done']:\n",
    "#         return {\n",
    "#                 'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "#                 'current_step': '__end__',\n",
    "#                 }\n",
    "\n",
    "#     # In case the pentester should be run\n",
    "#     state['current_step'] = response['next_agent']\n",
    "#     if response['next_agent'] == 'pentester':\n",
    "#         return {\n",
    "#                 'messages': [{'role': 'assistant', \n",
    "#                              'content': response['supervisor_thought']}],\n",
    "#                 'current_step': 'pentester',\n",
    "#                 'pentest_tasks': response['tasks'],\n",
    "#                 }\n",
    "\n",
    "#     # Base condition (?)\n",
    "#     return {\n",
    "#                 'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "#                 'current_step': '__end__',\n",
    "#             }\n",
    "\n",
    "\n",
    "def supervisor(state: PentestState) -> PentestState:\n",
    "    # If all evaluator concludes all tasks are done -> Termination\n",
    "    evaluation = state.get('evaluation', {})\n",
    "    if evaluation and 'end' in evaluation:\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "            'current_step': '__end__',\n",
    "        }\n",
    "\n",
    "    messages = [system_prompt] + state.get('messages', [])\n",
    "\n",
    "    # Pentester ran, and evaluation has been made (Yet done)\n",
    "    if evaluation:\n",
    "        messages = [system_prompt] + [HumanMessage(\n",
    "            content=f\"based on {evaluation}, what should we do now to finish the tasks: {state.get('pentest_tasks', [])}\"\n",
    "        )]\n",
    "\n",
    "    model = _get_model().with_structured_output(SupervisorTask)\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(messages)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant', 'content': f'Model error: {e}'}],\n",
    "            'current_step': '__end__',\n",
    "        }\n",
    "\n",
    "    # If supervisor concludes all tasks are done -> Termination\n",
    "    if response.get('done'):\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "            'current_step': '__end__',\n",
    "        }\n",
    "\n",
    "    # In case the pentester should be run\n",
    "    next_agent = response.get('next_agent')\n",
    "    if next_agent == 'pentester':\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant', \n",
    "                          'content': response.get('supervisor_thought', '')}],\n",
    "            'current_step': 'pentester',\n",
    "            'pentest_tasks': response.get('tasks', []),\n",
    "        }\n",
    "\n",
    "    # Base condition (fallback)\n",
    "    return {\n",
    "        'messages': [{'role': 'assistant', 'content': 'done, bye!'}],\n",
    "        'current_step': '__end__',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261e1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorder.py\n",
    "class Report(TypedDict):\n",
    "    report: Annotated[str, \"This is the summary written in Latex\"]\n",
    "    file_name: Annotated[str, \"This is the filename of the Latex report\"]\n",
    "\n",
    "\n",
    "\n",
    "def recorder_summary(state: PentestState, model):\n",
    "    prompt = '''You are tasked with recording and summarizing information of a chat between a human and an LLM in \\\n",
    "    which the LLM is utilizing the command line to execute tasks. You have as an input the history of the last command that ran \\\n",
    "    which inclueds the output logs and the message prompt for previous commands:\n",
    "     \n",
    "    <history>\n",
    "    {history}\n",
    "    </history>\n",
    "\n",
    "    Generate a summary for this using latex.\n",
    "    '''\n",
    "    formats = {\n",
    "            'history': {'user_prompts': state['messages']} | state['pentest_results'],\n",
    "        }\n",
    "    prompt = prompt.format(**formats)\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    state['recorder_options']['file_names'].append(response.file_name)\n",
    "    \n",
    "    with open(response.file_name, 'w') as f:\n",
    "        f.write(response.report)\n",
    "\n",
    "    return state\n",
    "\n",
    "def recorder_final(state: PentestState, model):\n",
    "    prompt = '''You are tasked with summarizing and reporting information of a chat between a human and an LLM in \\\n",
    "    whihc the LLM is utilizing the command line to execute tasks. You have as input the history of summaries of all previous \\\n",
    "    outputs and interactions between the human and the LLM:\n",
    "\n",
    "    <history>\n",
    "    {history}\n",
    "    </history>\n",
    "\n",
    "    You are tasked with generating a final report in a Latex format. Also return the file path as a relative path.\n",
    "    '''\n",
    "\n",
    "    summaries = []\n",
    "    file_names = state['recorder_options']['file_names']\n",
    "    for file_name in file_names:\n",
    "        with open(file_name) as f:\n",
    "            summary = f.read()\n",
    "            summaries.append(summary)\n",
    "\n",
    "    formats = {\n",
    "            'history': summaries\n",
    "        }\n",
    "    prompt = prompt.format(**formats)\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    state['recorder_options']['report'] = response.report\n",
    "    with open(response.file_name, 'w') as f:\n",
    "        f.write(response.report)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "# def recorder(state: PentestState):\n",
    "#     model = _get_model().with_structured_output(Report)\n",
    "#     generate_final_report = state['recorder_options']['generate_final_report']\n",
    "\n",
    "#     if generate_final_report:\n",
    "#         return recorder_final(state, model)\n",
    "#     else:\n",
    "#         state = recorder_summary(state, model)\n",
    "#         return recorder_final(state, model)\n",
    "\n",
    "def recorder(state: PentestState):\n",
    "    model = _get_model().with_structured_output(Report)\n",
    "\n",
    "    recorder_options = state.get('recorder_options', {})\n",
    "    generate_final_report = recorder_options.get('generate_final_report', False)\n",
    "\n",
    "    if generate_final_report:\n",
    "        return recorder_final(state, model)\n",
    "    else:\n",
    "        state = recorder_summary(state, model)\n",
    "        return recorder_final(state, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f08e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentester.py\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from typing_extensions import Annotated, TypedDict, List, Literal, Optional \n",
    "\n",
    "\n",
    "class PentesterResults(TypedDict):\n",
    "    \"\"\"The results that the pentester needs to work\"\"\"\n",
    "    message: Annotated[str, ..., \"Pentester thought process\"]\n",
    "    phase: Annotated[str, ..., \"Current phase of pentesting, such as scanning or exploitation\"]\n",
    "    tasks: Annotated[List[str], ..., \"Tasks to perform to complete the current phase\"]\n",
    "    results: Annotated[List[str], ..., \"Summary of results\"]\n",
    "    tool_use: Annotated[bool, ..., \"Indicates if tools are still needed to complete a task\"]\n",
    "    software: Annotated[List[str], ..., \"program names that will be used to perform the tasks\"]\n",
    "    command: Annotated[str, ..., \"possible command that will be run\"]\n",
    "\n",
    "\n",
    "def _swap_messages(messages):\n",
    "    new_messages = []\n",
    "    for m in messages:\n",
    "        if m['role'] == 'assistant':\n",
    "            new_messages.append({\"role\": \"user\", \"content\": m['content']})\n",
    "        else:\n",
    "            new_messages.append({\"role\": \"assistant\", \"content\": m['content']})\n",
    "    return new_messages\n",
    "\n",
    "\n",
    "# def pentester(state: PentestState) -> PentestState:\n",
    "#     # print(state)\n",
    "#     tasks = f\"\"\"\n",
    "#     Do the following tasks:\n",
    "\n",
    "#     {state['pentest_tasks']}\n",
    "#     \"\"\"\n",
    "\n",
    "#     if state['tool_results']: # remove later, for debugging\n",
    "#         return {'current_step': 'evaluator'}\n",
    "\n",
    "#     # messages = [system_prompt, HumanMessage(content=tasks)] \n",
    "#     messages = [system_prompt] + [{'role': 'user', 'content': tasks}] + _swap_messages(state['messages'])\n",
    "#     model = _get_model().with_structured_output(PentesterResults)\n",
    "#     response = model.invoke(messages)\n",
    "#     print(f'pentester:\\n{response = }')\n",
    "\n",
    "#     # In case that the tool needs to be used\n",
    "#     if response['tool_use']:\n",
    "#         return {\n",
    "#                 'messages': [{'role': 'assistant',\n",
    "#                               'content': response['message']}],\n",
    "#                               # 'tool_calls': response['software']}], \n",
    "#                 'current_step': 'pentester',\n",
    "#                 'tool_name': response['software'][0],\n",
    "#                 'task': response['tasks'][0],\n",
    "#                 # \"command\": \"nmap -sV -sC -p- -oN metasploitable2_scan.txt 192.168.100.231\",\n",
    "#                 \"command\": response['command'],\n",
    "#                 }\n",
    "#     return {'current_step': 'evaluator'}\n",
    "\n",
    "\n",
    "def pentester(state: PentestState) -> PentestState:\n",
    "    tasks_content = state.get('pentest_tasks', [])\n",
    "    tool_results = state.get('tool_results', [])\n",
    "    previous_messages = state.get('messages', [])\n",
    "\n",
    "    tasks = f\"\"\"\n",
    "    Do the following tasks:\n",
    "\n",
    "    {tasks_content}\n",
    "    \"\"\"\n",
    "\n",
    "    # Temporary shortcut for debugging\n",
    "    if tool_results:\n",
    "        return {'current_step': 'evaluator'}\n",
    "\n",
    "    # Build the message list safely\n",
    "    messages = [system_prompt] + [{'role': 'user', 'content': tasks}] + _swap_messages(previous_messages)\n",
    "\n",
    "    model = _get_model().with_structured_output(PentesterResults)\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(messages)\n",
    "        # print(f'pentester:\\n{response = }')\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant', 'content': f'Model error: {e}'}],\n",
    "            'current_step': 'evaluator'\n",
    "        }\n",
    "\n",
    "    # In case that the tool needs to be used\n",
    "    if response.get('tool_use'):\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant',\n",
    "                          'content': response.get('message', '')}],\n",
    "            'current_step': 'pentester',\n",
    "            'tool_name': response.get('software', [None])[0],\n",
    "            'task': response.get('tasks', [None])[0],\n",
    "            'command': response.get('command'),\n",
    "        }\n",
    "\n",
    "    return {'current_step': 'evaluator'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1689b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools.py\n",
    "class ToolUsage(TypedDict):\n",
    "    \"\"\"The task that the tool needs to do\"\"\"\n",
    "    message: str\n",
    "    # phase: Annotated[str, ... , \"Current phase of pentesting\"]\n",
    "    # task: Annotated[str, ... , \"Task to perform so that the tool agent succeeds.\"]\n",
    "    # hints: Annotated[Optional[List[str]], ... , \"May contain hints about the task\"]\n",
    "    # constraints: Annotated[Optional[List[str]], ... , \"Contains constraints that the tool user agent should be aware of\"]\n",
    "    # program_name: Annotated[str, ... , \"Program name that the tool uses to finish a task\"]\n",
    "    # args: Annotated[Optional[Dict], ... , \"Args that might help the tool user agent\"]\n",
    "    # results: Annotated[Dict, ... , \"Title of findings as keys, and a description as their value\"]\n",
    "    results: str\n",
    "\n",
    "\n",
    "def tools_node(state: PentestState) -> PentestState:\n",
    "    system_prompt = SystemMessage(f\"\"\"\n",
    "    You are an AI agent who is an expert in using a tool named {state['tool_name']}, \n",
    "    to do {state['pentest_tasks']}. You are working with a team of AI agents, \n",
    "    you will receive a task, and possibly hints, constrains, and args. \n",
    "    You will have to use your expertise in {state['tool_name']} to finish\n",
    "    the task. When you are done pass the results to the calling agent. \n",
    "\n",
    "    you have access to bash shell using shell_tool\n",
    "    \"\"\")\n",
    "    # print(f'{system_prompt = }')\n",
    "    human_msg = HumanMessage(content=f\"Use {state['tool_name']} to do {state['task']}\")\n",
    "    messages = [system_prompt] + [human_msg]\n",
    "    model = _get_model().bind_tools(tools)\n",
    "    ai_msg = model.invoke(messages)\n",
    "    messages.append(ai_msg)\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        selected_tool = {tool.name: tool for tool in tools}[tool_call[\"name\"].lower()]\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        # tool_msg = selected_tool.invoke(tool_call)\n",
    "        messages.append(tool_msg)\n",
    "    # tool_msg = shell_tool.invoke(ai_msg.tool_calls)\n",
    "    # messages.append(tool_msg)\n",
    "    messages.append(HumanMessage(content='summarize before parsing'))\n",
    "    model = _get_model().with_structured_output(ToolUsage)\n",
    "    response = model.invoke(messages)\n",
    "    # print(f'tools:\\n{response = }')\n",
    "    return {\n",
    "            'messages': [{'role': 'assistant',\n",
    "                          'content': '\\n\\n'.join(response.values())}],\n",
    "            'tool_results': response['results'],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0384d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eval(TypedDict):\n",
    "    \"\"\"Task evaluation\"\"\"\n",
    "    message: Annotated[str, ..., \"The message to respond to the user\"]\n",
    "    evaluation: Annotated[str, ..., \"Evaluate the pentester response, see if he finished the task\"]\n",
    "    done: Annotated[bool, ..., \"True if the pentester finished the task\"]\n",
    "    fail: Annotated[bool, ..., \"True if the pentester can't finish the task, this will end the program\"]\n",
    "    findings: Annotated[Dict[Literal['critical', 'high', 'medium', 'low'], List[str]], ..., \"denotes findings worth of mention\"]\n",
    "    # message: str\n",
    "    # phase: Annotated[str, ... , \"Current phase of pentesting\"]\n",
    "    # task: Annotated[str, ... , \"Task to perform so that the tool agent succeeds.\"]\n",
    "    # hints: Annotated[Optional[List[str]], ... , \"May contain hints about the task\"]\n",
    "    # constraints: Annotated[Optional[List[str]], ... , \"Contains constraints that the tool user agent should be aware of\"]\n",
    "    # program_name: Annotated[str, ... , \"Program name that the tool uses to finish a task\"]\n",
    "    # args: Annotated[Optional[Dict], ... , \"Args that might help the tool user agent\"]\n",
    "    # results: Annotated[Dict, ... , \"Title of findings as keys, and a description as their value\"]\n",
    "    # results: str\n",
    "\n",
    "def _swap_messages(messages):\n",
    "    new_messages = []\n",
    "    for m in messages:\n",
    "        if m['role'] == 'assistant':\n",
    "            new_messages.append({\"role\": \"user\", \"content\": m['content']})\n",
    "        else:\n",
    "            new_messages.append({\"role\": \"assistant\", \"content\": m['content']})\n",
    "    return new_messages\n",
    "\n",
    "# def evaluator(state: PentestState) -> PentestState:\n",
    "#     system_message = SystemMessage(\"\"\"\n",
    "#     You are evaluating the response from a pentester agent, \n",
    "#     if the pentester brings back some results, give him a pass,\n",
    "#     if he did very bad, and you think he can do better, point \n",
    "#     out his mistake and make him redo it again, if the pentester\n",
    "#     can't do the task. End the program, and let the supervisor know. \n",
    "#     \"\"\")\n",
    "\n",
    "#     model = _get_model().with_structured_output(Eval)\n",
    "#     messages = [system_message] + [HumanMessage(content=f\"evaluate pentester results {state['tool_results']}\")]\n",
    "#     # messages = [system_message] \\\n",
    "#         # + _swap_messages(state['messages']) \\\n",
    "#         # + [HumanMessage(content=f\"evaluate pentester results\")]\n",
    "#     response = model.invoke(messages)\n",
    "\n",
    "#     if response['done'] or response['fail']:\n",
    "#         response['evaluation'] += 'end program now'\n",
    "#     return {\n",
    "#             'messages': [{'role': 'assistant',\n",
    "#                          'content': response['message']}],\n",
    "#             'evaluation': response['evaluation'],\n",
    "#             'current_step': 'supervisor' if response['done'] or response['fail'] else 'pentester',\n",
    "#             # \"findings\": {\n",
    "#             #     \"critical\": ['4 vulnerabilities'],\n",
    "#             #     \"medium\": ['11 open ports'],\n",
    "#             #     },\n",
    "#             'findings': response['findings'],\n",
    "#     }\n",
    "\n",
    "\n",
    "def evaluator(state: PentestState) -> PentestState:\n",
    "    system_message = SystemMessage(\"\"\"\n",
    "    You are evaluating the response from a pentester agent, \n",
    "    if the pentester brings back some results, give him a pass,\n",
    "    if he did very bad, and you think he can do better, point \n",
    "    out his mistake and make him redo it again, if the pentester\n",
    "    can't do the task. End the program, and let the supervisor know. \n",
    "    \"\"\")\n",
    "\n",
    "    model = _get_model().with_structured_output(Eval)\n",
    "\n",
    "    tool_results = state.get('tool_results', [])\n",
    "    messages = [system_message, HumanMessage(content=f\"evaluate pentester results {tool_results}\")]\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(messages)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'messages': [{'role': 'assistant', 'content': f'Model error during evaluation: {e}'}],\n",
    "            'evaluation': 'evaluation failed due to error',\n",
    "            'current_step': 'supervisor',\n",
    "            'findings': {},\n",
    "        }\n",
    "\n",
    "    done = response.get('done', False)\n",
    "    fail = response.get('fail', False)\n",
    "    evaluation = response.get('evaluation', '')\n",
    "\n",
    "    if done or fail:\n",
    "        evaluation += ' end program now'\n",
    "\n",
    "    return {\n",
    "        'messages': [{'role': 'assistant', 'content': response.get('message', '')}],\n",
    "        'evaluation': evaluation,\n",
    "        'current_step': 'supervisor' if (done or fail) else 'pentester',\n",
    "        'findings': response.get('findings', {}),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb876b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def recorder(state: PentestState) -> PentestState:\n",
    "    # Logic for recorder node\n",
    "    return {'report': 'Pentest Report: Critical SQL Injection vulnerability found', 'current_step': 'end'}\n",
    "\n",
    "def route_supervisor(state: PentestState) -> Literal['pentester', 'evaluator', 'recorder', '__end__']:\n",
    "    return state['current_step']\n",
    "\n",
    "def route_pentester(state: PentestState) -> Literal['tools_node', 'evaluator']:\n",
    "    return 'tools_node' if state['current_step'] == 'pentester' else 'evaluator' \n",
    "\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(PentestState, config_schema=StateGraph)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node('supervisor', supervisor)\n",
    "workflow.add_node('pentester', pentester)\n",
    "workflow.add_node('tools_node', tools_node)\n",
    "workflow.add_node('evaluator', evaluator)\n",
    "workflow.add_node('recorder', recorder)\n",
    "\n",
    "# Add edges\n",
    "workflow.set_entry_point('supervisor')\n",
    "workflow.add_conditional_edges('supervisor', route_supervisor)\n",
    "workflow.add_conditional_edges('pentester', route_pentester)\n",
    "workflow.add_edge('tools_node', 'pentester')\n",
    "workflow.add_edge('evaluator', 'supervisor')\n",
    "workflow.add_edge('recorder', 'supervisor')\n",
    "\n",
    "# Compile the graph\n",
    "pentest_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0482d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query = \"Ping to 8.8.8.8 4 times, and check if the host is up.\"\n",
    "# # query = \"Visit https://dreamhack.io and tell me what this site is about.\"\n",
    "# query = \"Do fast port scanning on 10.10.11.38 to discover which tcp ports are open.\"\n",
    "\n",
    "# initial_state = {\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "#         }\n",
    "\n",
    "\n",
    "# for chunk in pentest_graph.stream(initial_state, stream_mode='values'):\n",
    "#     from pprint import pprint\n",
    "#     pprint(chunk['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aaf9d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_initial_state\u001b[39m(query):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pprint import pprint\n",
    "\n",
    "def create_initial_state(query):\n",
    "    return {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    }\n",
    "\n",
    "def stream_pentest(user_input, history):\n",
    "    initial_state = create_initial_state(user_input)\n",
    "\n",
    "    yield history + [[user_input, None]]\n",
    "\n",
    "    for chunk in pentest_graph.stream(initial_state, stream_mode='values'):\n",
    "        last_message = chunk['messages'][-1]['content']\n",
    "        history[-1][1] = last_message\n",
    "        yield history\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=stream_pentest,\n",
    "    chatbot=gr.Chatbot(),\n",
    "    title=\"Pentest Graph Chatbot\",\n",
    "    description=\"Ask a question like 'Scan 10.10.11.38 for open ports'\",\n",
    "    stop_btn=\"Stop\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
